# Ceph Hardware Recommendations

Ceph is designed to run efficiently on commodity hardware, which enables the building and maintenance of petabyte-scale data clusters in a cost-effective manner. When planning your Ceph hardware, it's crucial to consider various factors such as failure domains, performance issues, and distributing Ceph daemons across multiple hosts to optimize the cluster.

## Minimum Hardware Recommendations

Ceph can run successfully on inexpensive, commodity hardware. Even small production or development clusters can function well with modest resources. Below are the minimum hardware recommendations for the main Ceph daemons and processes.

### **1. Ceph OSD (Object Storage Daemon)**

| **Criteria**        | **Minimum Recommended**                              |
|---------------------|------------------------------------------------------|
| **Processor**       | 1 core minimum<br>1 core per 200-500 MB/s<br>1 core per 1000-3000 IOPS (before replication) |
| **RAM**             | 4GB+ per daemon (more is better)<br>2-4GB often functions (may be slow)<br>Less than 2GB is not recommended |
| **Volume Storage**  | 1x storage drive per daemon                          |
| **DB/WAL**          | 1x SSD partition per daemon (optional)              |
| **Network**         | 1x 1GbE+ NIC (10GbE+ recommended)                    |

**Notes**:
- Actual performance depends on several factors, including drive speed, network, and client throughput and latency.
- ARM processors may require additional cores.
- Benchmarking is highly recommended to assess performance.

### **2. Ceph MON (Monitor Daemon)**

| **Criteria**        | **Minimum Recommended**                              |
|---------------------|------------------------------------------------------|
| **Processor**       | 2 cores minimum                                      |
| **RAM**             | 24GB+ per daemon                                    |
| **Disk Space**      | 60GB per daemon                                      |
| **Network**         | 1x 1GbE+ NIC                                         |

### **3. Ceph MDS (Metadata Server)**

| **Criteria**        | **Minimum Recommended**                              |
|---------------------|------------------------------------------------------|
| **Processor**       | 2 cores minimum                                      |
| **RAM**             | 2GB+ per daemon                                     |
| **Disk Space**      | 1MB per daemon                                       |
| **Network**         | 1x 1GbE+ NIC                                         |

## Lab Hardware Requirements

For this lab, the following hardware configuration is recommended for each node:

### **Node 1: ceph1**

- **OS**: Ubuntu 22.04
- **Disks**:
  - `/dev/vda` (root): 20GB
  - `/dev/vdb`: 5GB
  - `/dev/vdc`: 5GB

### **Node 2: ceph2**

- **OS**: Ubuntu 22.04
- **Disks**:
  - `/dev/vda` (root): 20GB
  - `/dev/vdb`: 5GB
  - `/dev/vdc`: 5GB

### **Node 3: ceph3**

- **OS**: Ubuntu 22.04
- **Disks**:
  - `/dev/vda` (root): 20GB
  - `/dev/vdb`: 5GB
  - `/dev/vdc`: 5GB

### **General Notes:**
- **Storage**: At least one disk per daemon for the OSDs, and SSDs are recommended for database and WAL storage.
- **Networking**: A minimum of 1GbE NICs for network communication. 10GbE or higher is recommended for larger production clusters.
- **Performance**: The exact performance of Ceph clusters will depend on a variety of factors, including the drives used, network configuration, and client workload. Regular benchmarking is advised to ensure optimal performance.

## Conclusion

While Ceph can run on commodity hardware, it's important to plan for the right balance of resources across the cluster. This includes ensuring that the hardware meets the demands of each Ceph daemon (OSD, MON, MDS) and the required storage, CPU, memory, and network requirements are met. The recommended hardware configurations will help in building efficient and scalable Ceph clusters.
