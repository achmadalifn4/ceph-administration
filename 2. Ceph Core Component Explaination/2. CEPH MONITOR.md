# Ceph Monitor (ceph-mon)

Ceph Monitors (ceph-mon) are a critical component of the Ceph storage system, responsible for maintaining the **cluster map** and ensuring that the cluster's state remains consistent and up-to-date. These monitors play a key role in managing the internal metadata, tracking the state of cluster components, and ensuring that clients can easily access the right resources.

![Ceph Monitor Diagram](https://access.redhat.com/webassets/avalon/d/Red_Hat_Ceph_Storage-4-Configuration_Guide-en-US/images/da8c304521e1a81a44daabffc9c049fd/110_Ceph_Configuration_updates_0720_03.png)

## Key Functions of Ceph Monitors

- **Cluster Map Management**: Ceph Monitors maintain the "main copy" of the **Cluster Map**, which tracks the state of various components within the Ceph cluster, including:
  - **Monitors**
  - **Managers**
  - **Storage devices (OSDs)**
  - **Metadata servers (MDS)**
  - **System design and failure domains**

  This map enables Ceph clients to efficiently locate resources in the cluster, making it highly scalable and fast.

- **Authorization & Authentication**: Monitors handle authorization and authentication by managing which entities (such as users, clients, and nodes) are allowed to communicate with the system. This is vital for maintaining security in the cluster.

- **State Logging & Consistency**: Every change in the cluster state is logged by the monitors. This ensures that all updates to the cluster map are recorded, enabling consistency across the cluster. Ceph Monitors use a **key-value store** to maintain this state and ensure synchronization.

- **Quorum-Based Decision Making**: Ceph Monitors use a **quorum** system to make sure that changes to the cluster map are agreed upon by a majority of monitors. This ensures the reliability and consistency of the cluster map even if some monitors are unavailable.

## Cluster Maps

The cluster map is a comprehensive representation of the current state of the Ceph Storage Cluster. It includes the following key components:

- **Monitor Map**: Tracks the state of the monitors in the cluster.
- **OSD Map**: Shows the status of the OSD (Object Storage Daemon) devices.
- **Placement Group Map**: Tracks the health of placement groups (PGs), which are collections of objects managed by OSDs.
- **Metadata Server Map**: Contains information about the state of metadata servers in a CephFS deployment.

These maps help Ceph monitor the health and functioning of the cluster, keeping track of which components are working and which are not. When changes occur, such as a failure of an OSD or a PG experiencing issues, the maps are updated to reflect the new state.

Each version of these maps is referred to as an **epoch**, which keeps a history of changes and allows Ceph to track the evolution of the cluster state over time.

## Monitor Quorum

While a single monitor can work in a Ceph cluster, it poses a risk because if that monitor fails, the whole cluster could become unavailable. To ensure high availability and resilience, Ceph clusters are typically configured with multiple monitors. The monitors use the **Paxos** algorithm to reach an agreement on the cluster map.

A **quorum** is the minimum number of monitors required to agree on changes to the cluster map. If the number of working monitors falls below the quorum, the cluster can no longer make changes to the map, protecting the system from potential data corruption.

For example:
- With **3 monitors**, at least **2** must agree on the cluster map to form a quorum.
- With **5 monitors**, at least **3** must agree on the cluster map.

This ensures that even if one or two monitors fail, the cluster remains operational and consistent.

### Example: Checking Monitor Quorum

When you have 3 Ceph Monitors, you can check the quorum status by using the following command:

```bash
ceph quorum_status
