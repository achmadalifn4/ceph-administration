# Object Storage Daemon (OSD)

The **Object Storage Daemon (ceph-osd)** is one of the core components of a Ceph cluster. It is responsible for managing data storage, ensuring redundancy through replication, and helping to maintain the overall health and balance of the cluster. Each OSD typically corresponds to a single disk or storage device and interacts closely with other Ceph components, like **Ceph Monitors** and **Ceph Managers**, to share its status and maintain synchronization within the cluster.

## Key Functions of Ceph OSDs

- **Data Storage and Replication**: OSDs store the actual data in the form of objects. They replicate data across multiple devices to ensure high availability and fault tolerance.
- **Data Integrity**: OSDs perform integrity checks to ensure that the data stored is accurate and consistent.
- **Communication with Other Daemons**: OSDs interact with Ceph Monitors and Managers to update cluster maps and report their health status.
- **Self-healing and Recovery**: If an OSD fails or data becomes corrupt, the OSD daemon participates in the recovery process by rebalancing data and replacing lost copies through other OSDs.

### OSD Backends

There are two primary backends for managing data in a Ceph OSD:

1. **BlueStore (Recommended)**
2. **FileStore (Legacy)**

#### BlueStore

Introduced in the **Luminous 12.2.z** release, **BlueStore** is now the default and recommended backend for Ceph OSDs. It is specifically designed to provide better performance and simplified management compared to FileStore.

**Key Features of BlueStore**:
- **Direct Management of Storage Devices**: BlueStore directly handles raw block devices or partitions, bypassing the need for local file systems (such as XFS), which can limit performance.
- **Metadata Management with RocksDB**: Internal metadata (e.g., object name to block location mappings) is managed using RocksDB, a high-performance key-value store.
- **Data and Metadata Checksumming**: BlueStore protects all data and metadata with checksums to verify their integrity during read and write operations.
- **Inline Compression**: BlueStore can compress data before writing it to disk, helping to save storage space.
- **Multi-device Metadata Tiering**: BlueStore supports placing its internal write-ahead log (WAL) and other metadata on faster devices (e.g., SSDs or NVMe), improving performance for high-throughput applications.
- **Efficient Copy-on-write**: Snapshots and cloning, especially in **RBD (RADOS Block Device)** and **CephFS**, are handled more efficiently, using BlueStore's copy-on-write mechanism.

**BlueStore Architecture**:
- **Block**: The main storage device for BlueStore, which stores both the data and metadata.
- **Block.wal**: A **write-ahead log (WAL)** device, which stores write logs before they are committed to the main data device. This improves performance when the WAL device (e.g., SSD) is faster than the main device (e.g., HDD).
- **Block.db**: A **database device** used to store the internal metadata (using RocksDB). Storing this metadata on a faster device enhances performance by reducing latency in metadata lookups.

#### FileStore

Before **BlueStore**, the default storage backend for Ceph was **FileStore**, which used a file system (typically XFS) to store objects. FileStore also employed a special database for storing metadata, such as the mapping from objects to disk locations. 

**Key Features of FileStore**:
- **Uses File Systems**: FileStore relies on a traditional file system (typically XFS) for object storage.
- **Metadata Storage**: Originally, FileStore used **LevelDB** (now RocksDB) to store metadata, but the use of a regular file system for data storage limited performance and scalability.
- **Performance Limitations**: Due to the overhead of using file systems, FileStore can be slower, especially when handling large datasets or large numbers of objects. It is not as efficient or optimized for modern hardware as BlueStore.

**Recommendation**: While **FileStore** can still be used, **BlueStore** is the recommended and preferred backend in modern Ceph deployments due to its better performance, simpler architecture, and more efficient handling of storage devices.

## Summary of OSD Components

An OSD typically involves the following components:

1. **Block (Main Storage Device)**: The device used for storing both data and metadata.
2. **Block.wal (Write-Ahead Log Device)**: An optional device that serves as the journal for write operations, improving write performance when it is placed on faster devices like SSDs.
3. **Block.db (Database Device)**: An optional device used to store metadata (using RocksDB). This improves metadata lookup and IO performance when placed on faster devices.

### When to Use Which Backend:
- **BlueStore**: Use BlueStore for the best performance and flexibility, especially with modern hardware setups. It works well with raw block devices and supports high-speed metadata tiering.
- **FileStore**: FileStore is still functional but is generally only recommended for legacy systems or where BlueStore is not an option (e.g., with older Ceph versions prior to Luminous).

## Conclusion

The **Ceph OSD** is crucial for data storage and replication in a Ceph cluster. With the introduction of **BlueStore**, the performance of Ceph storage has significantly improved, making it the default and recommended backend. By managing raw devices directly and using efficient techniques like metadata tiering and inline compression, **BlueStore** delivers superior speed, reliability, and scalability compared to the older **FileStore** backend.
