## Lab 1.2 - Install & Configure Ceph

**Preparation**

Before you begin, log in using the `nusactl login` command with your ADINUSA credentials.

```
student@ceph1:~$ nusactl login
```

After login, run the `nusactl start cpadm-001-2` command. This command runs the start script and pre-configures your lab environment.

```
student@ceph1:~$ nusactl start cpadm-001-2
```

**Instructions**

**Execute on pod-username-ceph1**

1. Download and install cephadm:

   ```
   su - ceph-adm

   # Add keys
   wget -q -O- 'https://download.ceph.com/keys/release.asc' | sudo apt-key add -

   # Add repository
   echo deb https://download.ceph.com/debian-reef/ $(lsb_release -sc) main | sudo tee /etc/apt/sources.list.d/ceph.list

   # Update repository
   sudo apt update

   # Install cephadm
   sudo apt install -y cephadm
   ```

2. Ceph Initial Setup:

   ```
   # Create directory if it doesn't exist
   if [[ ! -d "/etc/ceph" ]]; then sudo mkdir -p /etc/ceph; fi

   # Run ceph bootstrap
   sudo cephadm bootstrap --mon-ip 10.XX.XX.10 --initial-dashboard-user adinusa88 --initial-dashboard-password adinusa88 --dashboard-password-noupdate

   **Note:** Get the fsid after bootstrap is done.

   # Enter Ceph shell and exit
   sudo cephadm shell --fsid <fsid> -c /etc/ceph/ceph.conf -k /etc/ceph/ceph.client.admin.keyring
   exit
   ```

3. Install ceph-common packages:

   **Note:** Change the root user password on all nodes before executing the command below.

   ```
   sudo cephadm install ceph-common
   sudo ceph version

   # Copy SSH key to other nodes
   ssh-copy-id -f -i /etc/ceph/ceph.pub root@pod--ceph2
   ssh-copy-id -f -i /etc/ceph/ceph.pub root@pod--ceph3
   ```

4. Add hosts to the cluster:

   ```
   sudo ceph orch host add pod--ceph2
   sudo ceph orch host add pod--ceph3
   ```

5. Add hosts to monitor:

   **Choose one of the following options:**

   * **Set a specific IP subnet for monitors:**

     ```
     sudo ceph config set mon public_network 10.XX.XX.0/24
     ```

   * **Deploy monitors on specific hosts:**

     ```
     sudo ceph orch apply mon "pod--ceph1,pod--ceph2,pod--ceph3"
     ```

   * **Set monitor labels on specific hosts:**

     ```
     sudo ceph orch host label add pod--ceph1 mon
     sudo ceph orch host label add pod--ceph2 mon
     sudo ceph orch host label add pod--ceph3 mon
     ```

   * **View current hosts and labels:**

     ```
     sudo ceph orch host ls
     ```

6. Create OSDs:

   ```
   # Add OSDs on specific devices
   sudo ceph orch daemon add osd pod--ceph1:/dev/vdb
   sudo ceph orch daemon add osd pod--ceph1:/dev/vdc
   sudo ceph orch daemon add osd pod--ceph2:/dev/vdb
   sudo ceph orch daemon add osd pod--ceph2:/dev/vdc
   sudo ceph orch daemon add osd pod--ceph3:/dev/vdb
   sudo ceph orch daemon add osd pod--ceph3:/dev/vdc
   ```

7. Verification:

   * Check OSD tree:

     ```
     sudo ceph osd tree
     ```

   * Check Ceph status:

     ```
     sudo ceph -s
     ```

   * Check detailed Ceph status:

     ```
     sudo ceph health detail
     sudo ceph mon stat
     ```

   * Check Ceph service containers:

     ```
     sudo ceph orch ps
     sudo docker ps
     ```
